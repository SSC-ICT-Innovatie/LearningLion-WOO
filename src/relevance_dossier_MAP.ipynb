{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import auc\n",
    "import os\n",
    "\n",
    "EVALUATION_FOLDER = \"final_evaluation_minbzk\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_minbzk_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_minbzk_keywords_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_minbzk_paraphrase_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_minbzk_real_words_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_real_words_minbzk_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_real_words_minbzk_keywords_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_real_words_minbzk_paraphrase_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_real_words_minbzk_real_words_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_stem_stopwords_minbzk_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_stem_stopwords_minbzk_keywords_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_stem_stopwords_minbzk_paraphrase_BM25S.csv\"\n",
    "# RESULTS_FILE = \"evaluation_minbzk_no_requests_stem_stopwords_minbzk_real_words_BM25S.csv\"\n",
    "\n",
    "RESULTS_FILE = \"evaluation_minbzk_no_requests_minbzk_paraphrase_all-MiniLM-L6-v2.csv\"\n",
    "\n",
    "SAVE_FOLDER = os.path.join(\"final_plots\", \"minbzk\")\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)\n",
    "\n",
    "woo_data = pd.read_csv(f\"./{EVALUATION_FOLDER}/results/{RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_map(n):\n",
    "    \"\"\"\n",
    "    Where n is the number of pages retrieved (max 100).\n",
    "    \"\"\"\n",
    "    results_summary = []\n",
    "    # thresholds = [i/100 for i in range(101)]\n",
    "    # Create 100 steps between 0 and 0.1\n",
    "    thresholds = [i * 0.0001 for i in range(1000)]\n",
    "\n",
    "    # Add values 0.2, 0.3, ..., up to 1\n",
    "    thresholds.extend([i * 0.1 for i in range(2, 11)])\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        headers = ['dossier_id', 'retrieved_document_ids', 'confidences', 'retrieved_document_ids_above_threshold', 'confusion_category']\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        for index, row in woo_data.iterrows():\n",
    "            document_ids = row[\"retrieved_document_ids\"].split(\", \")[:n]\n",
    "            unique_document_ids_set = set(document_ids)\n",
    "            unique_document_ids_dict = {key: 0 for key in unique_document_ids_set}\n",
    "            # If only 1 document has been retrieved, set confidence to 1\n",
    "            if len(unique_document_ids_dict) <= 1:\n",
    "                # Set confidence to 1\n",
    "                result = {key: 1 for key in unique_document_ids_dict}\n",
    "            else:\n",
    "                # Iterate over the dossier# columns\n",
    "                for key in unique_document_ids_dict:\n",
    "                    for idx, document_id in enumerate(document_ids):\n",
    "                        if key != document_id:\n",
    "                            continue\n",
    "                #         unique_document_ids_dict[key] += 1\n",
    "                # result = {key: value / sum(unique_document_ids_dict.values()) for key, value in unique_document_ids_dict.items()}\n",
    "                        unique_document_ids_dict[key] += document_ids[0:idx + 1].count(document_id)/(idx + 1)\n",
    "                \n",
    "                # Normalize values\n",
    "                result = {key: value / len(document_ids) for key, value in unique_document_ids_dict.items()}\n",
    "\n",
    "            # print(result)\n",
    "            retrieved_document_ids_above_threshold = [key for key, value in result.items() if value > threshold]\n",
    "            confusion_category = []\n",
    "            # Loop over the result list and check if the substring is in each key\n",
    "            for key, value in result.items():\n",
    "                if row['dossier_id'] in key and value >= threshold:\n",
    "                    confusion_category.append(\"tp\")\n",
    "                elif row['dossier_id'] in key and value <= threshold:\n",
    "                    confusion_category.append(\"fn\")\n",
    "                elif row['dossier_id'] not in key and value >= threshold:\n",
    "                    confusion_category.append(\"fp\")\n",
    "                else:\n",
    "                    confusion_category.append(\"tn\")\n",
    "\n",
    "            df.loc[len(df)] = [\n",
    "                row['dossier_id'],\n",
    "                result.keys(),\n",
    "                result.values(),\n",
    "                \"N/A\" if len(retrieved_document_ids_above_threshold) == 0 else retrieved_document_ids_above_threshold,\n",
    "                confusion_category\n",
    "            ]\n",
    "\n",
    "        # Initialize a dictionary to store the counts\n",
    "        result = {'threshold': threshold, 'tp': 0, 'fn': 0, 'fp': 0, 'tn': 0}\n",
    "\n",
    "        # Flatten the list of lists and count occurrences of each category\n",
    "        for array in df['confusion_category']:\n",
    "            for item in array:\n",
    "                result[item] += 1\n",
    "        results_summary.append(result)\n",
    "    return results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# thresholds = [i/100 for i in range(101)]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Normal\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dossier_id_limits:\n\u001b[1;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(result)\n\u001b[0;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m, in \u001b[0;36mprocess_map\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m             confusion_category\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     53\u001b[0m         row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdossier_id\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     54\u001b[0m         result\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m     55\u001b[0m         result\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(retrieved_document_ids_above_threshold) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m retrieved_document_ids_above_threshold,\n\u001b[0;32m     57\u001b[0m         confusion_category\n\u001b[0;32m     58\u001b[0m     ]\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Initialize a dictionary to store the counts\u001b[39;00m\n\u001b[0;32m     61\u001b[0m result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m: threshold, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfn\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtn\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\Nicky\\Desktop\\Repositories\\LearningLion-WOO\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nicky\\Desktop\\Repositories\\LearningLion-WOO\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1932\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[0;32m   1931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m-> 1932\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nicky\\Desktop\\Repositories\\LearningLion-WOO\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:2328\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m   2326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_append\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_mgr\n\u001b[0;32m   2329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_maybe_update_cacher(clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Nicky\\Desktop\\Repositories\\LearningLion-WOO\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10572\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m  10569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m  10570\u001b[0m     to_concat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m, other]\n\u001b[1;32m> 10572\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10576\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10577\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nicky\\Desktop\\Repositories\\LearningLion-WOO\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nicky\\Desktop\\Repositories\\LearningLion-WOO\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\Nicky\\Desktop\\Repositories\\LearningLion-WOO\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dossier_id_limits = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "dossier_id_limits = [10, 50, 100]\n",
    "thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "# thresholds = [i/100 for i in range(101)]\n",
    "\n",
    "# Normal\n",
    "for i in dossier_id_limits:\n",
    "    result = process_map(i)\n",
    "    results_df = pd.DataFrame(result)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.plot(results_df['threshold'], results_df['tp'], marker='o', label='tp')\n",
    "    plt.plot(results_df['threshold'], results_df['fn'], marker='o', label='fn')\n",
    "    plt.plot(results_df['threshold'], results_df['fp'], marker='o', label='fp')\n",
    "    plt.plot(results_df['threshold'], results_df['tn'], marker='o', label='tn')\n",
    "    \n",
    "    plt.title(f'Results Summary for Different Thresholds with nr of pages retrieved = {i}')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xticks(thresholds)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    save_path = os.path.join(SAVE_FOLDER, f\"weighted_frequency_{RESULTS_FILE.split('.')[0]}_{i}.png\")\n",
    "    plt.savefig(save_path)\n",
    "    # plt.show()\n",
    "\n",
    "# ROC\n",
    "    # Calculate TPR and FPR\n",
    "    tpr = [d['tp'] / (d['tp'] + d['fn']) for d in result]\n",
    "    fpr = [(d['fp'] / (d['fp'] + d['tn']) if (d['fp'] + d['tn']) != 0 else 0) for d in result]\n",
    "\n",
    "    print(f\"i = {i}\")\n",
    "    print(result)\n",
    "\n",
    "    # Calculate AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, marker='o', label=f'ROC curve (AUC = {roc_auc:.8f})')\n",
    "    plt.title(f'ROC Curve Weighted Frequency for n = {i}')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    save_path = os.path.join(SAVE_FOLDER, f\"weighted_frequency_roc_{RESULTS_FILE.split('.')[0]}_{i}.png\")\n",
    "    plt.savefig(save_path)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'threshold': 0, 'tp': 98, 'fn': 0, 'fp': 1728, 'tn': 0},\n",
       " {'threshold': 0.01, 'tp': 98, 'fn': 0, 'fp': 1728, 'tn': 0},\n",
       " {'threshold': 0.02, 'tp': 69, 'fn': 29, 'fp': 979, 'tn': 749},\n",
       " {'threshold': 0.03, 'tp': 63, 'fn': 35, 'fp': 675, 'tn': 1053},\n",
       " {'threshold': 0.04, 'tp': 59, 'fn': 39, 'fp': 497, 'tn': 1231},\n",
       " {'threshold': 0.05, 'tp': 54, 'fn': 44, 'fp': 465, 'tn': 1263}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_map_calc_confusion(n=10):\n",
    "    \"\"\"\n",
    "    Where n is the number of pages retrieved (max 100).\n",
    "    \"\"\"\n",
    "    results_summary = []\n",
    "    thresholds = [0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        headers = ['dossier_id', 'retrieved_document_ids', 'confidences', 'retrieved_document_ids_above_threshold', 'confusion_category']\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        for index, row in woo_data.iterrows():\n",
    "            document_ids = row[\"retrieved_document_ids\"].split(\", \")[:n]\n",
    "            unique_document_ids_set = set(document_ids)\n",
    "            unique_document_ids_dict = {key: 0 for key in unique_document_ids_set}\n",
    "            # If only 1 document has been retrieved, set confidence to 1\n",
    "            if len(unique_document_ids_dict) <= 1:\n",
    "                # Set confidence to 1\n",
    "                result = {key: 1 for key in unique_document_ids_dict}\n",
    "            else:\n",
    "                # Iterate over the dossier# columns\n",
    "                for key in unique_document_ids_dict:\n",
    "                    for idx, document_id in enumerate(document_ids):\n",
    "                        if key != document_id:\n",
    "                            continue\n",
    "                #         unique_document_ids_dict[key] += 1\n",
    "                # result = {key: value / sum(unique_document_ids_dict.values()) for key, value in unique_document_ids_dict.items()}\n",
    "                        unique_document_ids_dict[key] += document_ids[0:idx + 1].count(document_id)/(idx + 1)\n",
    "                \n",
    "                # Normalize values\n",
    "                result = {key: value / len(document_ids) for key, value in unique_document_ids_dict.items()}\n",
    "\n",
    "            # print(result)\n",
    "            retrieved_document_ids_above_threshold = [key for key, value in result.items() if value > threshold]\n",
    "            confusion_category = []\n",
    "            # Loop over the result list and check if the substring is in each key\n",
    "            for key, value in result.items():\n",
    "                if row['dossier_id'] in key and value >= threshold:\n",
    "                    confusion_category.append(\"tp\")\n",
    "                elif row['dossier_id'] in key and value <= threshold:\n",
    "                    confusion_category.append(\"fn\")\n",
    "                elif row['dossier_id'] not in key and value >= threshold:\n",
    "                    confusion_category.append(\"fp\")\n",
    "                else:\n",
    "                    confusion_category.append(\"tn\")\n",
    "\n",
    "            df.loc[len(df)] = [\n",
    "                row['dossier_id'],\n",
    "                result.keys(),\n",
    "                result.values(),\n",
    "                \"N/A\" if len(retrieved_document_ids_above_threshold) == 0 else retrieved_document_ids_above_threshold,\n",
    "                confusion_category\n",
    "            ]\n",
    "\n",
    "        # Initialize a dictionary to store the counts\n",
    "        result = {'threshold': threshold, 'tp': 0, 'fn': 0, 'fp': 0, 'tn': 0}\n",
    "\n",
    "        # Flatten the list of lists and count occurrences of each category\n",
    "        for array in df['confusion_category']:\n",
    "            for item in array:\n",
    "                result[item] += 1\n",
    "        results_summary.append(result)\n",
    "    return results_summary\n",
    "\n",
    "process_map_calc_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
